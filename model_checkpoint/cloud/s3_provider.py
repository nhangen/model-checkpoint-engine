"""Optimized S3 provider implementation - zero redundancy design"""

from typing import Dict, List, Any, Optional
import json

from .base_provider import (
    BaseCloudProvider, CloudCredentials, CloudObject, UploadResult, DownloadResult
)


class S3Provider(BaseCloudProvider):
    """Optimized AWS S3 provider with efficient operations"""

    def __init__(self, credentials: CloudCredentials, bucket_name: str):
        """
        Initialize S3 provider

        Args:
            credentials: AWS credentials
            bucket_name: S3 bucket name
        """
        super().__init__(credentials, bucket_name)

        # Optimized: S3-specific settings
        self._storage_class = 'STANDARD'
        self._server_side_encryption = 'AES256'

    def _create_client(self):
        """Create S3 client - optimized with optional dependencies"""
        try:
            import boto3
            from botocore.config import Config

            # Optimized: Configure with retry and timeout settings
            config = Config(
                region_name=self.credentials.region,
                retries={'max_attempts': 3, 'mode': 'adaptive'},
                max_pool_connections=self._max_concurrency
            )

            # Optimized: Support different credential methods
            if self.credentials.access_key and self.credentials.secret_key:
                client = boto3.client(
                    's3',
                    aws_access_key_id=self.credentials.access_key,
                    aws_secret_access_key=self.credentials.secret_key,
                    aws_session_token=self.credentials.session_token,
                    endpoint_url=self.credentials.endpoint_url,
                    config=config
                )
            else:
                # Use default credential chain (IAM roles, environment, etc.)
                client = boto3.client('s3', config=config)

            return client

        except ImportError:
            raise ImportError("boto3 is required for S3 provider. Install with: pip install boto3")

    def _upload_file_impl(self, local_path: str, cloud_key: str,
                         metadata: Optional[Dict[str, Any]] = None) -> UploadResult:
        """S3-specific file upload implementation"""
        try:
            client = self.get_client()

            # Optimized: Prepare upload parameters
            extra_args = {
                'StorageClass': self._storage_class,
                'ServerSideEncryption': self._server_side_encryption
            }

            if metadata:
                # S3 metadata keys must be lowercase and prefixed
                s3_metadata = {}
                for key, value in metadata.items():
                    s3_key = f"x-amz-meta-{key.lower().replace('_', '-')}"
                    s3_metadata[s3_key] = str(value)
                extra_args['Metadata'] = s3_metadata

            # Optimized: Use multipart upload for large files
            import os
            file_size = os.path.getsize(local_path)

            if file_size > self._multipart_threshold:
                # Use multipart upload
                response = client.upload_file(
                    local_path,
                    self.bucket_name,
                    cloud_key,
                    ExtraArgs=extra_args
                )
            else:
                # Use simple upload
                with open(local_path, 'rb') as f:
                    response = client.put_object(
                        Bucket=self.bucket_name,
                        Key=cloud_key,
                        Body=f,
                        **extra_args
                    )

            # Get object metadata for ETag
            head_response = client.head_object(Bucket=self.bucket_name, Key=cloud_key)

            return UploadResult(
                success=True,
                key=cloud_key,
                etag=head_response.get('ETag', '').strip('"'),
                version_id=head_response.get('VersionId')
            )

        except Exception as e:
            return UploadResult(
                success=False,
                key=cloud_key,
                error=str(e)
            )

    def _download_file_impl(self, cloud_key: str, local_path: str) -> DownloadResult:
        """S3-specific file download implementation"""
        try:
            client = self.get_client()

            # Optimized: Download with progress tracking
            client.download_file(self.bucket_name, cloud_key, local_path)

            return DownloadResult(
                success=True,
                key=cloud_key
            )

        except Exception as e:
            return DownloadResult(
                success=False,
                key=cloud_key,
                error=str(e)
            )

    def _list_objects_impl(self, prefix: str = "", max_keys: int = 1000) -> List[CloudObject]:
        """S3-specific object listing implementation"""
        try:
            client = self.get_client()
            objects = []

            # Optimized: Use paginator for large result sets
            paginator = client.get_paginator('list_objects_v2')
            page_iterator = paginator.paginate(
                Bucket=self.bucket_name,
                Prefix=prefix,
                MaxKeys=max_keys
            )

            for page in page_iterator:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        # Optimized: Parse S3 response efficiently
                        cloud_obj = CloudObject(
                            key=obj['Key'],
                            size=obj['Size'],
                            last_modified=obj['LastModified'].timestamp(),
                            etag=obj.get('ETag', '').strip('"'),
                            storage_class=obj.get('StorageClass', 'STANDARD')
                        )
                        objects.append(cloud_obj)

                        if len(objects) >= max_keys:
                            break

                if len(objects) >= max_keys:
                    break

            return objects

        except Exception as e:
            print(f"S3 list objects failed: {e}")
            return []

    def _delete_object_impl(self, cloud_key: str) -> bool:
        """S3-specific object deletion implementation"""
        try:
            client = self.get_client()
            client.delete_object(Bucket=self.bucket_name, Key=cloud_key)
            return True

        except Exception as e:
            print(f"S3 delete object failed: {e}")
            return False

    def _object_exists_impl(self, cloud_key: str) -> bool:
        """S3-specific object existence check implementation"""
        try:
            client = self.get_client()
            client.head_object(Bucket=self.bucket_name, Key=cloud_key)
            return True

        except Exception:
            return False

    def verify_upload(self, local_path: str, cloud_key: str) -> bool:
        """
        Verify S3 upload integrity using ETag comparison

        Args:
            local_path: Local file path
            cloud_key: S3 object key

        Returns:
            True if checksums match
        """
        try:
            import os

            if not os.path.exists(local_path) or not self.object_exists(cloud_key):
                return False

            client = self.get_client()

            # Get S3 object ETag
            response = client.head_object(Bucket=self.bucket_name, Key=cloud_key)
            s3_etag = response.get('ETag', '').strip('"')

            if not s3_etag:
                return False

            # Calculate local file MD5 (S3 ETag for single-part uploads)
            file_size = os.path.getsize(local_path)

            if file_size <= self._multipart_threshold and '-' not in s3_etag:
                # Single-part upload - ETag is MD5
                local_md5 = self.calculate_local_checksum(local_path, 'md5')
                return local_md5 == s3_etag
            else:
                # Multipart upload - cannot verify with simple MD5
                # For now, just check if object exists and has reasonable size
                return response.get('ContentLength', 0) == file_size

        except Exception as e:
            print(f"S3 upload verification failed: {e}")
            return False

    def set_storage_class(self, storage_class: str) -> None:
        """
        Set S3 storage class for uploads

        Args:
            storage_class: S3 storage class (STANDARD, STANDARD_IA, GLACIER, etc.)
        """
        valid_classes = [
            'STANDARD', 'REDUCED_REDUNDANCY', 'STANDARD_IA', 'ONEZONE_IA',
            'INTELLIGENT_TIERING', 'GLACIER', 'DEEP_ARCHIVE', 'GLACIER_IR'
        ]

        if storage_class in valid_classes:
            self._storage_class = storage_class
        else:
            raise ValueError(f"Invalid storage class: {storage_class}")

    def set_encryption(self, encryption: str, kms_key_id: Optional[str] = None) -> None:
        """
        Set S3 server-side encryption

        Args:
            encryption: Encryption type (AES256, aws:kms, aws:kms:dsse)
            kms_key_id: KMS key ID for KMS encryption
        """
        valid_encryptions = ['AES256', 'aws:kms', 'aws:kms:dsse']

        if encryption in valid_encryptions:
            self._server_side_encryption = encryption
            if encryption.startswith('aws:kms') and kms_key_id:
                self._kms_key_id = kms_key_id
        else:
            raise ValueError(f"Invalid encryption type: {encryption}")

    def create_presigned_url(self, cloud_key: str, expiration: int = 3600,
                           http_method: str = 'GET') -> Optional[str]:
        """
        Create presigned URL for S3 object

        Args:
            cloud_key: S3 object key
            expiration: URL expiration time in seconds
            http_method: HTTP method (GET, PUT, etc.)

        Returns:
            Presigned URL or None if error
        """
        try:
            client = self.get_client()

            if http_method == 'GET':
                url = client.generate_presigned_url(
                    'get_object',
                    Params={'Bucket': self.bucket_name, 'Key': cloud_key},
                    ExpiresIn=expiration
                )
            elif http_method == 'PUT':
                url = client.generate_presigned_url(
                    'put_object',
                    Params={'Bucket': self.bucket_name, 'Key': cloud_key},
                    ExpiresIn=expiration
                )
            else:
                return None

            return url

        except Exception as e:
            print(f"Failed to create presigned URL: {e}")
            return None

    def cleanup_failed_uploads(self) -> int:
        """
        Clean up failed multipart uploads in S3

        Returns:
            Number of uploads cleaned up
        """
        try:
            client = self.get_client()
            cleaned_count = 0

            # List incomplete multipart uploads
            response = client.list_multipart_uploads(Bucket=self.bucket_name)

            if 'Uploads' in response:
                for upload in response['Uploads']:
                    upload_id = upload['UploadId']
                    key = upload['Key']

                    try:
                        # Abort the multipart upload
                        client.abort_multipart_upload(
                            Bucket=self.bucket_name,
                            Key=key,
                            UploadId=upload_id
                        )
                        cleaned_count += 1

                    except Exception as e:
                        print(f"Failed to abort upload {upload_id}: {e}")

            return cleaned_count

        except Exception as e:
            print(f"Failed to cleanup multipart uploads: {e}")
            return 0

    def copy_object(self, source_key: str, dest_key: str,
                   source_bucket: Optional[str] = None) -> bool:
        """
        Copy object within S3 or between buckets

        Args:
            source_key: Source object key
            dest_key: Destination object key
            source_bucket: Source bucket (None = same bucket)

        Returns:
            True if successful
        """
        try:
            client = self.get_client()
            source_bucket = source_bucket or self.bucket_name

            copy_source = {
                'Bucket': source_bucket,
                'Key': source_key
            }

            client.copy_object(
                CopySource=copy_source,
                Bucket=self.bucket_name,
                Key=dest_key,
                StorageClass=self._storage_class,
                ServerSideEncryption=self._server_side_encryption
            )

            return True

        except Exception as e:
            print(f"S3 copy failed: {e}")
            return False

    def get_object_metadata(self, cloud_key: str) -> Optional[Dict[str, Any]]:
        """
        Get detailed S3 object metadata

        Args:
            cloud_key: S3 object key

        Returns:
            Metadata dictionary or None
        """
        try:
            client = self.get_client()
            response = client.head_object(Bucket=self.bucket_name, Key=cloud_key)

            # Optimized: Extract relevant metadata
            metadata = {
                'size': response.get('ContentLength', 0),
                'last_modified': response.get('LastModified'),
                'etag': response.get('ETag', '').strip('"'),
                'content_type': response.get('ContentType', ''),
                'storage_class': response.get('StorageClass', 'STANDARD'),
                'server_side_encryption': response.get('ServerSideEncryption'),
                'version_id': response.get('VersionId'),
                'metadata': response.get('Metadata', {})
            }

            return metadata

        except Exception as e:
            print(f"Failed to get S3 object metadata: {e}")
            return None